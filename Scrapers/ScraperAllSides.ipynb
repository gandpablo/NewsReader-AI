{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bc2a3-7c95-4b79-bcd9-95749ed2ca99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def get_driver(link):\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-popup-blocking\")\n",
    "    \n",
    "    \n",
    "    prefs = {\n",
    "        \"profile.default_content_setting_values.popups\": 0,\n",
    "        \"profile.default_content_settings.popups\": 0,\n",
    "        \"profile.managed_default_content_settings.popups\": 0,\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    driver = uc.Chrome(options=options)\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "    \n",
    "    driver.get(link)\n",
    "    wait.until(EC.visibility_of_element_located((By.CLASS_NAME, 'views-field-name')))\n",
    "    \n",
    "    return driver,wait\n",
    "\n",
    "def get_elements1(driver):\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    tabla = soup.find('table', class_='views-table cols-3')\n",
    "    links = tabla.find_all('tr')\n",
    "\n",
    "    urls = []\n",
    "    topics = []\n",
    "    dates = []\n",
    "    \n",
    "    for link in links:\n",
    "\n",
    "        td1 = link.find('td', class_='views-field views-field-name')\n",
    "        a_tag1 = td1.find('a') if td1 else None\n",
    "        url = a_tag1['href'] if a_tag1 else None\n",
    "        if url:\n",
    "            url = 'https://www.allsides.com'+url\n",
    "    \n",
    "        td2 = link.find('td', class_='views-field views-field-field-story-topic')\n",
    "        a_tag2 = td2.find('a') if td2 else None\n",
    "        topic = a_tag2['href'] if a_tag2 else None\n",
    "        topic = topic.split('/')[-1] if topic else None\n",
    "    \n",
    "        td3 = link.find('td', class_='views-field views-field-field-story-date')\n",
    "        span = td3.find('span',class_='date-display-single') if td3 else None\n",
    "        date = span.text if span else None\n",
    "    \n",
    "        \n",
    "        if url and topic and date:\n",
    "            \n",
    "            urls.append(url)\n",
    "            topics.append(topic)\n",
    "            dates.append(date)\n",
    "\n",
    "    return urls,topics,dates\n",
    "\n",
    "def get_sides(driver, url):\n",
    "    driver.execute_script(\"window.open('');\")\n",
    "    tabs = driver.window_handles\n",
    "    driver.switch_to.window(tabs[-1])\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'featured-coverage'))\n",
    "        )\n",
    "    except:\n",
    "        driver.close()\n",
    "        driver.switch_to.window(tabs[0])\n",
    "        return None, None, None\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    driver.close()\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    content = soup.find('div', class_='featured-coverage')\n",
    "    if not content:\n",
    "        return None, None, None\n",
    "\n",
    "    left = content.find('div', class_='news-item left')\n",
    "    center = content.find('div', class_='news-item center')\n",
    "    right = content.find('div', class_='news-item right')\n",
    "\n",
    "    return left, center, right\n",
    "\n",
    "\n",
    "def get_elements2(elemento):\n",
    "    \n",
    "    a1 = elemento.find('a',class_='news-title')\n",
    "    new_url = a1['href']\n",
    "    new_title = a1.text\n",
    "    \n",
    "    a2 = elemento.find('a',class_='source-area')\n",
    "    site = a2.find('div',class_='news-source').text\n",
    "    \n",
    "    type = a2.find('img')['src']\n",
    "    type = type.split('/')[-1].replace('bias-','').replace('.png','')\n",
    "    \n",
    "    if 'x' in type:\n",
    "        parts = type.split('-')\n",
    "        parts = [p for p in parts if 'x' not in p]\n",
    "        type = '-'.join(parts)\n",
    "\n",
    "    return new_url, new_title,site,type\n",
    "\n",
    "def action(driver):\n",
    "\n",
    "    urls_global = []\n",
    "    topics_global = []\n",
    "    dates_global = []\n",
    "    title_global = []\n",
    "    site_global = []\n",
    "    type_global = []\n",
    "    \n",
    "    urls,topics,dates = get_elements1(driver)\n",
    "    \n",
    "    total = len(urls)\n",
    "    lleva = 0\n",
    "\n",
    "    for url,topic,date in zip(urls,topics,dates):\n",
    "        print(f'URL {lleva} de {total}')\n",
    "        lleva += 1\n",
    "        try:\n",
    "            left,center,right = get_sides(driver,url)\n",
    "            time.sleep(0.1)\n",
    "            sides = [left,center,right]\n",
    "\n",
    "            for side in sides:\n",
    "                try:\n",
    "                    new_url, new_title,site,type = get_elements2(side)\n",
    "        \n",
    "                    urls_global.append(new_url)\n",
    "                    topics_global.append(topic)\n",
    "                    dates_global.append(date)\n",
    "                    title_global.append(new_title)\n",
    "                    site_global.append(site)\n",
    "                    type_global.append(type)\n",
    "                except:\n",
    "                    print(f'Ha fallado en {side}')\n",
    "        except:\n",
    "            print('Una noticia fallida')\n",
    "\n",
    "    \n",
    "            \n",
    "    df = pd.DataFrame({\n",
    "        'url': urls_global,\n",
    "        'topic': topics_global,\n",
    "        'date': dates_global,\n",
    "        'title': title_global,\n",
    "        'site': site_global,\n",
    "        'bias': type_global\n",
    "    })\n",
    "\n",
    "    \n",
    "    return df   \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    i = 115\n",
    "    errores = 0\n",
    "    scrolls = 0\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    while errores < 20:\n",
    "\n",
    "        link = f'https://www.allsides.com/headline-roundups?page={i}'\n",
    "        i += 1\n",
    "        \n",
    "        try:\n",
    "            driver,wait = get_driver(link)\n",
    "            print(f'\\nINICIANDO SCROLL {scrolls}')\n",
    "            print('-'*60)\n",
    "            print()\n",
    "            \n",
    "            df2 = action(driver)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "            df.to_csv('/Users/pablogandia/Desktop/allsides_links_moment.csv',index=False)\n",
    "            driver.quit()\n",
    "            time.sleep(1)\n",
    "            scrolls += 1\n",
    "            \n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            print()\n",
    "            print('-'*60)\n",
    "            print('ERROR FUERTE DETECTADO')\n",
    "            print('-'*60)\n",
    "            \n",
    "            errores += 1\n",
    "\n",
    "    df.to_csv('/Users/pablogandia/Desktop/allsides_links_def.csv',index=False)\n",
    "    driver.quit()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e67c89-e8a8-4a1e-a108-294766ca2c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22287"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/pablogandia/Desktop/allsides_links_def.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aeeb317-aed9-488f-a90a-fabfab8aa975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22264"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['url'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010739fa-8a1c-4730-8c36-23403c468ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>topic</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>site</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.npr.org/2021/01/01/952336030/what-...</td>\n",
       "      <td>general-news</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>What Got Us Through 2020? For Many, It Was Hob...</td>\n",
       "      <td>NPR (Online News)</td>\n",
       "      <td>leaning-left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.washingtonpost.com/opinions/2020/1...</td>\n",
       "      <td>general-news</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>Goodbye, 2020. You were awful, but some lights...</td>\n",
       "      <td>Guest Writer - Right</td>\n",
       "      <td>leaning-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cbsnews.com/news/new-years-eve-gat...</td>\n",
       "      <td>general-news</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>New Year's Eve gatherings could accelerate COV...</td>\n",
       "      <td>CBS News (Online)</td>\n",
       "      <td>leaning-left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.ibtimes.com/nye-traditions-you-can...</td>\n",
       "      <td>general-news</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>NYE Traditions That You Can Still Participate ...</td>\n",
       "      <td>International Business Times</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://nypost.com/2020/12/31/heres-how-countr...</td>\n",
       "      <td>general-news</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>Here’s how countries around the world are ring...</td>\n",
       "      <td>New York Post (News)</td>\n",
       "      <td>leaning-right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url         topic  \\\n",
       "0  https://www.npr.org/2021/01/01/952336030/what-...  general-news   \n",
       "1  https://www.washingtonpost.com/opinions/2020/1...  general-news   \n",
       "2  https://www.cbsnews.com/news/new-years-eve-gat...  general-news   \n",
       "3  https://www.ibtimes.com/nye-traditions-you-can...  general-news   \n",
       "4  https://nypost.com/2020/12/31/heres-how-countr...  general-news   \n",
       "\n",
       "         date                                              title  \\\n",
       "0  2021-01-01  What Got Us Through 2020? For Many, It Was Hob...   \n",
       "1  2021-01-01  Goodbye, 2020. You were awful, but some lights...   \n",
       "2  2020-12-31  New Year's Eve gatherings could accelerate COV...   \n",
       "3  2020-12-31  NYE Traditions That You Can Still Participate ...   \n",
       "4  2020-12-31  Here’s how countries around the world are ring...   \n",
       "\n",
       "                           site           bias  \n",
       "0             NPR (Online News)   leaning-left  \n",
       "1          Guest Writer - Right  leaning-right  \n",
       "2             CBS News (Online)   leaning-left  \n",
       "3  International Business Times         center  \n",
       "4          New York Post (News)  leaning-right  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
